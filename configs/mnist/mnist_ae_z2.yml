#############################################
#                                           #
# code from IRVAE_public (Lee et al., 2022) #
#                                           #
#############################################

logdir: 'results/MNIST/AE'
logger: 
  type: base
  endwith: ['@']
model:
  arch: ae 
  encoder:
    arch: fc_image
    l_hidden: [256, 256, 256, 256, ]
    activation: ['relu', 'relu', 'relu', 'relu', ]
    out_activation: 'linear'
    img_size: [1, 28, 28]
    out_chan_num: 1
  decoder:
    arch: fc_image
    l_hidden: [256, 256, 256, 256, ]
    activation: ['relu', 'relu', 'relu', 'relu', ]
    out_activation: 'sigmoid'
    img_size: [1, 28, 28]
    out_chan_num: 1
  x_dim: 784
  z_dim: 2                   
data:
  training:
    dataset: MNIST              
    root: dataset
    batch_size: 10                        # number of time series in batch
    n_workers: 4
    split: training
    shuffle: True

    digits: [3]
    time_horizon: 36                      # number of images in a series
    precompute_global_dist: False
  
  validation:
    dataset: MNIST               
    root: dataset
    batch_size: 10
    n_workers: 4
    split: validation
    shuffle: True

    digits: [3]
    time_horizon: 36
    precompute_global_dist: False

trainer: graph                      
training:
  n_epoch: 100
  optimizer:
    name: 'adam'
    lr: 0.0001
  print_interval: 100
  val_interval: 500
  visualize_interval: 500
  seed: 0
  model_seed: 0