#############################################
#                                           #
# code from IRVAE_public (Lee et al., 2022) #
#                                           #
#############################################

logdir: 'results/RotatingMNIST/TS_GGAE'
logger: 
  type: base
  endwith: ['@']
model:
  arch: timeseriesggae 
  encoder:
    arch: fc_image
    l_hidden: [256, 256, 256, 256, ]
    activation: ['relu', 'relu', 'relu', 'relu', ]
    out_activation: 'linear'
    img_size: [1, 28, 28]
    out_chan_num: 1
  decoder:
    arch: fc_image
    l_hidden: [256, 256, 256, 256, ]
    activation: ['relu', 'relu', 'relu', 'relu', ]
    out_activation: 'sigmoid'
    img_size: [1, 28, 28]
    out_chan_num: 1
  x_dim: 784
  z_dim: 2
  iso_reg: 0.1                          
  measure: 'isometry'
data:
  training:
    dataset: RotatingMNIST              
    root: dataset
    batch_size: 10                        # number of time series in batch
    n_workers: 4
    split: training
    shuffle: True

    digits: [3]
    time_horizon: 36                      # number of images in a series

    collate_fn: 'timeseries_laplacian_collate_fn' 
    collate_device: 'cuda:0'
    bandwidth: 200   
    precompute_global_dist: False
    distfunc: 'TimeSteps'

    data_GT_distfunc: 'TimeSteps'         # ground truth distance (for evaluation)
  
  validation:
    dataset: RotatingMNIST               
    root: dataset
    batch_size: 10
    n_workers: 4
    split: validation
    shuffle: True

    digits: [3]
    time_horizon: 36

    collate_fn: 'timeseries_laplacian_collate_fn' 
    collate_device: 'cuda:0'
    bandwidth: 200   
    precompute_global_dist: False
    distfunc: 'TimeSteps'
    
    data_GT_distfunc: 'TimeSteps'

trainer: graph                      
training:
  n_epoch: 100
  optimizer:
    name: 'adam'
    lr: 0.0001
  print_interval: 100
  val_interval: 500
  visualize_interval: 500
  seed: 0
  model_seed: 0